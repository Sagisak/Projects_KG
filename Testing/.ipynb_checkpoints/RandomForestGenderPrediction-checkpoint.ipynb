{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9adbb11d-18a7-430c-a4d1-dc382d3fb9dc",
   "metadata": {},
   "source": [
    "# Random Forest Model for classifying Gender by Indonesian names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a54f2d-0bd7-420f-941a-576a6a8a0557",
   "metadata": {},
   "source": [
    "## Import necessary files needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ebb7fd2-1e44-486f-af66-431dc86fde42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from random_forest_cart import RandomForestCART\n",
    "from random_forest_cart import CARTTree\n",
    "# Load the trained model and preprocessing tools\n",
    "rf = joblib.load(\"random_forest_cart.pkl\")\n",
    "vectorizer = joblib.load(\"vectorizer.pkl\")\n",
    "selector = joblib.load(\"selector.pkl\")\n",
    "scaler = joblib.load(\"scaler.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0987b60f-9cf8-4d6a-baac-431c36c9d648",
   "metadata": {},
   "source": [
    "## Two option for model that needed to be run first:\n",
    "1. Retrieve and Inserting Data from SQL server\n",
    "2. Retrieve the data from an CSV and save it into a CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e729e3ab-2d43-45f2-ab6b-592a24e450bb",
   "metadata": {},
   "source": [
    "### OPTION 1: Retrieve and Inserting data from SQL server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b20864d-35f5-4862-8cc6-03205b352f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "\n",
    "# Define the function to fetch and preprocess data\n",
    "def preprocessing_dataset():\n",
    "    print(\"Connecting to SQL Server...\")\n",
    "    try:\n",
    "        # Define connection string (replace with your actual SQL Server details)\n",
    "        conn = pyodbc.connect(\n",
    "            \"DRIVER={SQL Server};\"\n",
    "            \"SERVER=10.12.30.240;\"\n",
    "            \"DATABASE=GORPDWHBI;\"  # Replace with your database name\n",
    "            \"UID=viewer;\"\n",
    "            \"PWD=viewer1;\"\n",
    "        )\n",
    "        print(\"Successfully connected to SQL Server!\")\n",
    "\n",
    "        # Query to fetch the required columns\n",
    "        query = \"\"\"\n",
    "        WITH RankedCustomers AS (\n",
    "            SELECT \n",
    "                MyValueId, \n",
    "                CustomerName, \n",
    "                Gender,\n",
    "                ROW_NUMBER() OVER (PARTITION BY CustomerName ORDER BY MyValueId ASC) AS row_num\n",
    "            FROM dbo.DimCustomer\n",
    "            WHERE Gender NOT IN ('F', 'M')\n",
    "              AND MyValueId IS NOT NULL \n",
    "              AND CustomerName IS NOT NULL\n",
    "              AND LTRIM(RTRIM(MyValueId)) <> ''  \n",
    "              AND LTRIM(RTRIM(CustomerName)) <> ''  \n",
    "        )\n",
    "        SELECT MyValueId, CustomerName, Gender \n",
    "        FROM RankedCustomers\n",
    "        WHERE row_num = 1;\n",
    "         \"\"\"\n",
    "        print(\"Processing query...\")\n",
    "\n",
    "        # Read the data from SQL Server\n",
    "        GenderData = pd.read_sql(query, conn)\n",
    "        print(\"Successfully fetched data from SQL Server!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to connect or fetch data: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        # Close connection\n",
    "        conn.close()\n",
    "        print(\"Connection closed.\")\n",
    "\n",
    "    # Drop null values in 'CustomerName'\n",
    "    print(\"Cleaning data...\")\n",
    "    GenderData = GenderData.dropna(subset=['CustomerName'])\n",
    "    print(\"Data successfully cleaned!\")\n",
    "\n",
    "    # Function to split names\n",
    "    def split_name(name):\n",
    "        name = name.lower()\n",
    "        parts = name.split()\n",
    "        \n",
    "        first_name = second_name = third_name = fourth_name = last_name = ' '\n",
    "        \n",
    "        if len(parts) >= 1:\n",
    "            first_name = parts[0]\n",
    "        if len(parts) >= 2:\n",
    "            second_name = parts[1]\n",
    "        if len(parts) >= 3:\n",
    "            third_name = parts[2]\n",
    "        if len(parts) >= 4:\n",
    "            fourth_name = parts[3]\n",
    "        if len(parts) >= 5:\n",
    "            last_name = ' '.join(parts[4:])\n",
    "        \n",
    "        return pd.Series([first_name, second_name, third_name, fourth_name, last_name])\n",
    "    \n",
    "    # Apply function to DataFrame\n",
    "    print(\"Splitting customer names into individual parts...\")\n",
    "    GenderData[['FirstName', 'SecondName', 'ThirdName', 'FourthName', 'LastName']] = GenderData['CustomerName'].apply(split_name)\n",
    "    print(\"Name splitting completed!\")\n",
    "\n",
    "    # Drop original 'CustomerName' and 'Gender' columns\n",
    "    columns_to_drop = ['CustomerName', 'Gender']\n",
    "    GenderData = GenderData.drop(columns=columns_to_drop, errors='ignore')\n",
    "    \n",
    "    print(\"Preprocessing completed successfully!\")\n",
    "\n",
    "    return GenderData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f80e723-70e7-4a39-a200-4cc2f95691be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "def run_prediction():\n",
    "    start_algorithm_time = time.time()\n",
    "    print(\"Starting the algorithm...\")\n",
    "\n",
    "    # Log execution time\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(f\"Script executed at: {timestamp}\")\n",
    "\n",
    "    try:\n",
    "        # Fetch and preprocess data\n",
    "        start_prediction_time = time.time()\n",
    "        print(\"Starting the prediction process...\")\n",
    "        print(\"Fetching and preprocessing dataset from SQL Server...\")\n",
    "        GenderData = preprocessing_dataset()\n",
    "        if GenderData is None or GenderData.empty:\n",
    "            print(\"No data retrieved. Exiting process.\")\n",
    "            return\n",
    "        print(\"Data fetched and preprocessed successfully!\")\n",
    "\n",
    "        def get_bigrams(name):\n",
    "            return ' '.join(name[i:i+2] for i in range(len(name)-1)) if isinstance(name, str) else \"\"\n",
    "\n",
    "\n",
    "        # Join all name columns into a single column\n",
    "        X_new = GenderData[['FirstName', 'SecondName', 'ThirdName', 'FourthName', 'LastName']].astype(str)\n",
    "        GenderData[\"CustomerName\"] = X_new.apply(lambda row: ' '.join(row).strip(), axis=1)\n",
    "\n",
    "        # Extract bigrams\n",
    "        print(\"Extracting bigrams...\")\n",
    "        X_new = X_new.map(get_bigrams)\n",
    "        X_new[\"FullName\"] = X_new.apply(lambda row: ' '.join(row), axis=1)\n",
    "        print(\"Bigrams extracted!\")\n",
    "\n",
    "        # Apply HashingVectorizer\n",
    "        print(\"Applying feature transformation...\")\n",
    "        X_new_hashed = vectorizer.transform(X_new[\"FullName\"])  \n",
    "        X_new_df = pd.DataFrame(X_new_hashed.toarray())\n",
    "        print(\"Feature transformation completed!\")\n",
    "\n",
    "        # Remove low-variance features\n",
    "        print(\"Removing low-variance features...\")\n",
    "        X_new_df = pd.DataFrame(selector.transform(X_new_df))\n",
    "        print(\"Low-variance features removed!\")\n",
    "\n",
    "        # Scale features\n",
    "        print(\"Scaling features...\")\n",
    "        X_new_scaled = scaler.transform(X_new_df)\n",
    "        print(\"Feature scaling completed!\")\n",
    "\n",
    "        # Predict using the trained model\n",
    "        print(\"Model is predicting genders...\")\n",
    "        y_pred_new = rf.predict(X_new_scaled)\n",
    "        print(\"Prediction completed!\")\n",
    "\n",
    "        # Map predictions back to 'M' and 'F'\n",
    "        print(\"Mapping predictions to labels...\")\n",
    "        GenderData[\"Predicted_Gender\"] = np.where(y_pred_new == 0, 'M', 'F')\n",
    "        GenderData = GenderData[[\"MyValueId\", \"CustomerName\", \"Predicted_Gender\"]]\n",
    "        print(\"Mapping completed!\")\n",
    "\n",
    "        \n",
    "        end_prediction_time = time.time()\n",
    "        elapsed_prediction_time = end_prediction_time - start_prediction_time\n",
    "        minutes = int(elapsed_prediction_time // 60)  # Get whole minutes\n",
    "        seconds = int(elapsed_prediction_time % 60)   # Get remaining seconds\n",
    "\n",
    "        print(f\"Model estimated time completed: {minutes} minutes and {seconds} seconds\")\n",
    "        \n",
    "        print(\"Prediction process completed!\")\n",
    "        \n",
    "\n",
    "        # Show preview of mapped data\n",
    "        print(\"Preview of predicted data:\")\n",
    "        print(GenderData.head(10))  # Show first 10 rows\n",
    "        conn = pyodbc.connect(\n",
    "            \"DRIVER={SQL Server};\"\n",
    "            \"SERVER=10.12.30.240;\"\n",
    "            \"DATABASE=GORPDWHBI;\"\n",
    "            \"UID=viewer;\"\n",
    "            \"PWD=viewer1;\"\n",
    "        )\n",
    "        \n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        \n",
    "       # Loop through DataFrame rows and insert into SQL Server\n",
    "        # Loop through DataFrame rows and insert into SQL Server\n",
    "        print(\"Inserting predicted data into SQL Server...\")\n",
    "\n",
    "        if GenderData.empty:\n",
    "            print(\"GenderData is empty, nothing to insert.\")\n",
    "        else:\n",
    "            # Initialize counters\n",
    "            total_inserted = 0\n",
    "            commit_threshold = 1000  # Commit every 1000 rows\n",
    "            \n",
    "            # Get the total number of rows in GenderData\n",
    "            total_rows = len(GenderData)\n",
    "            start_time = time.time()\n",
    "            \n",
    "            for index, row in GenderData.iterrows():\n",
    "                try:\n",
    "                    # Print progress every 1000 rows (or adjust based on preference) and commit it\n",
    "                    if total_inserted % commit_threshold == 0 and total_inserted > 0:\n",
    "                        # Commit after 1000 rows\n",
    "                        conn.commit()\n",
    "                        print(f\"Committed {commit_threshold} rows to the database.\")\n",
    "                        \n",
    "                        # Calculate and print progress\n",
    "                        elapsed_time = time.time() - start_time\n",
    "                        progress_percentage = (total_inserted / total_rows) * 100  # Calculate progress as percentage\n",
    "                        print(f\"Progress: {total_inserted}/{total_rows} rows inserted ({progress_percentage:.2f}% done) in {elapsed_time:.2f} seconds\")\n",
    "                    \n",
    "                    # Execute stored procedure for each row\n",
    "                    cursor.execute(\"\"\"\n",
    "                    EXEC sp_GenderPredictionMember @MyValueId = ?, @CustomerName = ?, @Predicted_Gender = ?\n",
    "                    \"\"\",\n",
    "                    row['MyValueId'],  # MyValueId\n",
    "                    row['CustomerName'],  # Nama Customer\n",
    "                    row['Predicted_Gender']  # Hasil Prediksi Gender\n",
    "                    )\n",
    "                    \n",
    "                    total_inserted += 1\n",
    "            \n",
    "            \n",
    "                except Exception as e:\n",
    "                    print(f\"Error inserting row {index}: {e}\")\n",
    "            \n",
    "            # Final commit for any remaining data if total_inserted is not a multiple of 1000\n",
    "            if total_inserted % commit_threshold != 0:\n",
    "                conn.commit()\n",
    "                print(f\"Committed remaining {total_inserted % commit_threshold} rows to the database.\")\n",
    "            \n",
    "            print(f\"Total records inserted: {total_inserted}\")\n",
    "\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "            print(\"Data insertion process completed!\")\n",
    "\n",
    "\n",
    "        print(\"Data inserted successfully!\")\n",
    "        end_algorithm_time = time.time()\n",
    "        elapsed_time = end_algorithm_time - start_algorithm_time\n",
    "        minutes = int(elapsed_time // 60)  # Get whole minutes\n",
    "        seconds = int(elapsed_time % 60)   # Get remaining seconds\n",
    "\n",
    "        print(f\"Algorithm estimated time completed: {minutes} minutes and {seconds} seconds\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    print(\"Algorithm completed!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15e562b-e845-492c-867d-6e6a8c465d5b",
   "metadata": {},
   "source": [
    "### OPTION 2: Retrieve the data from an CSV and save it into a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddc2b7d-3602-4392-99ad-4c5fbb72af14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "\n",
    "# Define the function to fetch and preprocess data\n",
    "def preprocessing_dataset():\n",
    "    print(\"Taking data from dataset...\")\n",
    "    try:\n",
    "        GenderData = pd.read_csv(\"DatasetGenderNeededClarify.csv\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch data: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        print(\"Data get!\")\n",
    "\n",
    "    # Drop null values in 'CustomerName'\n",
    "    print(\"Cleaning data...\")\n",
    "    GenderData = GenderData.dropna(subset=['CustomerName'])\n",
    "    print(\"Data successfully cleaned!\")\n",
    "\n",
    "    # Function to split names\n",
    "    def split_name(name):\n",
    "        name = name.lower()\n",
    "        parts = name.split()\n",
    "        \n",
    "        first_name = second_name = third_name = fourth_name = last_name = ' '\n",
    "        \n",
    "        if len(parts) >= 1:\n",
    "            first_name = parts[0]\n",
    "        if len(parts) >= 2:\n",
    "            second_name = parts[1]\n",
    "        if len(parts) >= 3:\n",
    "            third_name = parts[2]\n",
    "        if len(parts) >= 4:\n",
    "            fourth_name = parts[3]\n",
    "        if len(parts) >= 5:\n",
    "            last_name = ' '.join(parts[4:])\n",
    "        \n",
    "        return pd.Series([first_name, second_name, third_name, fourth_name, last_name])\n",
    "    \n",
    "    # Apply function to DataFrame\n",
    "    print(\"Splitting customer names into individual parts...\")\n",
    "    GenderData[['FirstName', 'SecondName', 'ThirdName', 'FourthName', 'LastName']] = GenderData['CustomerName'].apply(split_name)\n",
    "    print(\"Name splitting completed!\")\n",
    "\n",
    "    # Drop original 'CustomerName' and 'Gender' columns\n",
    "    columns_to_drop = ['CustomerName', 'Gender']\n",
    "    GenderData = GenderData.drop(columns=columns_to_drop, errors='ignore')\n",
    "    \n",
    "    print(\"Preprocessing completed successfully!\")\n",
    "\n",
    "    return GenderData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15b6f3a-36c4-46ad-8d87-9f3f1be04ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "def run_prediction():\n",
    "    start_prediction_time = time.time()\n",
    "    print(\"Starting the prediction process...\")\n",
    "\n",
    "    # Log execution time\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(f\"Script executed at: {timestamp}\")\n",
    "\n",
    "    try:\n",
    "        # Fetch and preprocess data\n",
    "        print(\"Fetching and preprocessing dataset from csv...\")\n",
    "        GenderData = preprocessing_dataset()\n",
    "        if GenderData is None or GenderData.empty:\n",
    "            print(\"No data retrieved. Exiting process.\")\n",
    "            return\n",
    "        print(\"Data fetched and preprocessed successfully!\")\n",
    "\n",
    "        def get_bigrams(name):\n",
    "            return ' '.join(name[i:i+2] for i in range(len(name)-1)) if isinstance(name, str) else \"\"\n",
    "\n",
    "        # Join all name columns into a single column\n",
    "        X_new = GenderData[['FirstName', 'SecondName', 'ThirdName', 'FourthName', 'LastName']].astype(str)\n",
    "        GenderData[\"CustomerName\"] = X_new.apply(lambda row: ' '.join(row).strip(), axis=1)\n",
    "\n",
    "        # Extract bigrams\n",
    "        print(\"Extracting bigrams...\")\n",
    "        X_new = X_new.map(get_bigrams)\n",
    "        X_new[\"FullName\"] = X_new.apply(lambda row: ' '.join(row), axis=1)\n",
    "        print(\"Bigrams extracted!\")\n",
    "\n",
    "        # Apply HashingVectorizer\n",
    "        print(\"Applying feature transformation...\")\n",
    "        X_new_hashed = vectorizer.transform(X_new[\"FullName\"])  \n",
    "        X_new_df = pd.DataFrame(X_new_hashed.toarray())\n",
    "        print(\"Feature transformation completed!\")\n",
    "\n",
    "        # Remove low-variance features\n",
    "        print(\"Removing low-variance features...\")\n",
    "        X_new_df = pd.DataFrame(selector.transform(X_new_df))\n",
    "        print(\"Low-variance features removed!\")\n",
    "\n",
    "        # Scale features\n",
    "        print(\"Scaling features...\")\n",
    "        X_new_scaled = scaler.transform(X_new_df)\n",
    "        print(\"Feature scaling completed!\")\n",
    "\n",
    "        # Predict using the trained model\n",
    "        print(\"Model is predicting genders...\")\n",
    "        y_pred_new = rf.predict(X_new_scaled)\n",
    "        print(\"Prediction completed!\")\n",
    "\n",
    "        # Map predictions back to 'M' and 'F'\n",
    "        print(\"Mapping predictions to labels...\")\n",
    "        GenderData[\"Predicted_Gender\"] = np.where(y_pred_new == 0, 'M', 'F')\n",
    "        GenderData = GenderData[[\"MyValueId\", \"CustomerName\", \"Predicted_Gender\"]]\n",
    "        print(\"Mapping completed!\")\n",
    "\n",
    "        # Show preview of mapped data\n",
    "        print(\"Preview of predicted data:\")\n",
    "        print(GenderData.head(10))  # Show first 10 rows\n",
    "        \n",
    "        # Save output\n",
    "        output_filename = f\"GenderData_Predicted_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "        GenderData.head(100).to_csv(output_filename, index=False)\n",
    "        print(f\"Predicted data (first 100 rows) saved successfully as {output_filename}!\")\n",
    "        end_prediction_time = time.time()\n",
    "        elapsed_time = end_prediction_time - start_prediction_time\n",
    "        minutes = int(elapsed_time // 60)  # Get whole minutes\n",
    "        seconds = int(elapsed_time % 60)   # Get remaining seconds\n",
    "\n",
    "        print(f\"Testing Time: {minutes} minutes and {seconds} seconds\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    print(\"Prediction process completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be362b3c-54c2-42e0-ad00-a4735e0f0cdb",
   "metadata": {},
   "source": [
    "## Below code is too start the program!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c174b3d3-11f3-4a9a-95cd-c798726184a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "while True:\n",
    "    start_prediction_time = time.time()\n",
    "    run_prediction()\n",
    "    end_prediction_time = time.time()\n",
    "    print(f\"Testing Time: {end_prediction_time - start_prediction_time:.4f} seconds\")\n",
    "    \n",
    "    # Sleep for 15 hour (900 seconds)\n",
    "    \n",
    "    print(\"Sleeping for 15 minutes...\")\n",
    "    time.sleep(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbda083-30c8-4fdf-bf52-8042723f7d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import schedule\n",
    "\n",
    "# Schedule the job to run at a specific time (e.g., 15:00 or 3:00 PM)\n",
    "schedule.every().day.at(\"22:00\").do(run_prediction)\n",
    "\n",
    "while True:\n",
    "    # Run any pending scheduled tasks\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)  # Small sleep to prevent a busy loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887ba312-a0cc-414a-987e-a064decaaf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import schedule\n",
    "\n",
    "schedule.clear()\n",
    "\n",
    "\n",
    "# Schedule the job to run at 11:30 and 12:30 every day\n",
    "schedule.every().day.at(\"13:10\").do(run_prediction)\n",
    "\n",
    "# Run the scheduled tasks\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)  # Small sleep to prevent a busy loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82eee2db-5f8a-451c-9017-7ed23b970c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fcd08c93-c470-4bea-bd83-5d67c8f64cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the algorithm...\n",
      "Script executed at: 2025-02-12 09:43:11\n",
      "Starting the prediction process...\n",
      "Fetching and preprocessing dataset from SQL Server...\n",
      "Connecting to SQL Server...\n",
      "Successfully connected to SQL Server!\n",
      "Processing query...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_20088\\1914108838.py:40: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  GenderData = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully fetched data from SQL Server!\n",
      "Connection closed.\n",
      "Cleaning data...\n",
      "Data successfully cleaned!\n",
      "Splitting customer names into individual parts...\n",
      "Name splitting completed!\n",
      "Preprocessing completed successfully!\n",
      "Data fetched and preprocessed successfully!\n",
      "Extracting bigrams...\n",
      "Bigrams extracted!\n",
      "Applying feature transformation...\n",
      "Feature transformation completed!\n",
      "Removing low-variance features...\n",
      "Low-variance features removed!\n",
      "Scaling features...\n",
      "Feature scaling completed!\n",
      "Model is predicting genders...\n",
      "Prediction completed!\n",
      "Mapping predictions to labels...\n",
      "Mapping completed!\n",
      "Model estimated time completed: 16 minutes and 13 seconds\n",
      "Prediction process completed!\n",
      "Preview of predicted data:\n",
      "  MyValueId                 CustomerName Predicted_Gender\n",
      "0    VPHXVX               candra laksana                M\n",
      "1    VT1WMZ                      gunawan                M\n",
      "2    V5ZLL4  pristina nurdianti nasution                M\n",
      "3    VRXN9Q                  seni mulana                F\n",
      "4    VUZ0V7                sylvia thomas                F\n",
      "5    VPYHQ2                  ajum jumana                M\n",
      "6    VKIKZT                chintya laras                F\n",
      "7    VVXYKL                     giskaini                F\n",
      "8    V2JE59          putri ayu anggraini                M\n",
      "9    VEXAVW          yusni bin abd ghani                M\n",
      "Inserting predicted data into SQL Server...\n",
      "Committed 1000 rows to the database.\n",
      "Progress: 1000/2025861 rows inserted (0.05% done) in 7.30 seconds\n",
      "Committed 1000 rows to the database.\n",
      "Progress: 2000/2025861 rows inserted (0.10% done) in 11.70 seconds\n",
      "Committed 1000 rows to the database.\n",
      "Progress: 3000/2025861 rows inserted (0.15% done) in 16.48 seconds\n",
      "Committed 1000 rows to the database.\n",
      "Progress: 4000/2025861 rows inserted (0.20% done) in 22.17 seconds\n",
      "Committed 1000 rows to the database.\n",
      "Progress: 5000/2025861 rows inserted (0.25% done) in 28.88 seconds\n",
      "Committed 1000 rows to the database.\n",
      "Progress: 6000/2025861 rows inserted (0.30% done) in 35.02 seconds\n",
      "Committed 1000 rows to the database.\n",
      "Progress: 7000/2025861 rows inserted (0.35% done) in 45.03 seconds\n",
      "Committed 1000 rows to the database.\n",
      "Progress: 8000/2025861 rows inserted (0.39% done) in 53.82 seconds\n",
      "Committed 1000 rows to the database.\n",
      "Progress: 9000/2025861 rows inserted (0.44% done) in 62.32 seconds\n",
      "Committed 1000 rows to the database.\n",
      "Progress: 10000/2025861 rows inserted (0.49% done) in 67.82 seconds\n",
      "Committed 1000 rows to the database.\n",
      "Progress: 11000/2025861 rows inserted (0.54% done) in 72.33 seconds\n",
      "Committed 1000 rows to the database.\n",
      "Progress: 12000/2025861 rows inserted (0.59% done) in 78.60 seconds\n",
      "Committed 1000 rows to the database.\n",
      "Progress: 13000/2025861 rows inserted (0.64% done) in 84.77 seconds\n",
      "Committed 1000 rows to the database.\n",
      "Progress: 14000/2025861 rows inserted (0.69% done) in 90.69 seconds\n",
      "Committed 1000 rows to the database.\n",
      "Progress: 15000/2025861 rows inserted (0.74% done) in 99.00 seconds\n",
      "Committed 1000 rows to the database.\n",
      "Progress: 16000/2025861 rows inserted (0.79% done) in 107.22 seconds\n",
      "Committed 1000 rows to the database.\n",
      "Progress: 17000/2025861 rows inserted (0.84% done) in 115.49 seconds\n",
      "Committed 1000 rows to the database.\n",
      "Progress: 18000/2025861 rows inserted (0.89% done) in 122.44 seconds\n",
      "Committed 1000 rows to the database.\n",
      "Progress: 19000/2025861 rows inserted (0.94% done) in 129.91 seconds\n",
      "Committed 1000 rows to the database.\n",
      "Progress: 20000/2025861 rows inserted (0.99% done) in 139.15 seconds\n",
      "Committed 1000 rows to the database.\n",
      "Progress: 21000/2025861 rows inserted (1.04% done) in 146.03 seconds\n",
      "Committed 1000 rows to the database.\n",
      "Progress: 22000/2025861 rows inserted (1.09% done) in 153.99 seconds\n",
      "Committed 1000 rows to the database.\n",
      "Progress: 23000/2025861 rows inserted (1.14% done) in 163.79 seconds\n",
      "Committed 1000 rows to the database.\n",
      "Progress: 24000/2025861 rows inserted (1.18% done) in 173.19 seconds\n",
      "Committed 1000 rows to the database.\n",
      "Progress: 25000/2025861 rows inserted (1.23% done) in 181.00 seconds\n",
      "Committed 1000 rows to the database.\n",
      "Progress: 26000/2025861 rows inserted (1.28% done) in 190.29 seconds\n",
      "Committed 1000 rows to the database.\n",
      "Progress: 27000/2025861 rows inserted (1.33% done) in 196.97 seconds\n",
      "Committed 1000 rows to the database.\n",
      "Progress: 28000/2025861 rows inserted (1.38% done) in 208.20 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m run_prediction()\n",
      "Cell \u001b[1;32mIn[23], line 120\u001b[0m, in \u001b[0;36mrun_prediction\u001b[1;34m()\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProgress: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_inserted\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_rows\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m rows inserted (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprogress_percentage\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m% done) in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00melapsed_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;66;03m# Execute stored procedure for each row\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m     cursor\u001b[38;5;241m.\u001b[39mexecute(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;124m    EXEC sp_GenderPredictionMember @MyValueId = ?, @CustomerName = ?, @Predicted_Gender = ?\u001b[39m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m,\n\u001b[0;32m    123\u001b[0m     row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMyValueId\u001b[39m\u001b[38;5;124m'\u001b[39m],  \u001b[38;5;66;03m# MyValueId\u001b[39;00m\n\u001b[0;32m    124\u001b[0m     row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCustomerName\u001b[39m\u001b[38;5;124m'\u001b[39m],  \u001b[38;5;66;03m# Nama Customer\u001b[39;00m\n\u001b[0;32m    125\u001b[0m     row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted_Gender\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Hasil Prediksi Gender\u001b[39;00m\n\u001b[0;32m    126\u001b[0m     )\n\u001b[0;32m    128\u001b[0m     total_inserted \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a47177-e3c6-46e1-ac2a-fdfafffb0fd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
